{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import dataset_utils\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RANDOM_SEED = 0\n",
    "_NUM_VALIDATION = 5000#8550\n",
    "\n",
    "\n",
    "class JpegImageReader(object):\n",
    "    def __init__(self):\n",
    "        # Initializes function that decodes RGB JPEG data.\n",
    "        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "    def read_image_dims(self, sess, image_data):\n",
    "        image = self.decode_jpeg(sess, image_data)\n",
    "        return image.shape[0], image.shape[1]\n",
    "\n",
    "    def decode_jpeg(self, sess, image_data):\n",
    "        image = sess.run(self._decode_jpeg,\n",
    "                     feed_dict={self._decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "    \n",
    "class PngImageReader(object):\n",
    "    def __init__(self):\n",
    "        # Initializes function that decodes RGB JPEG data.\n",
    "        self._decode_png_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_png = tf.image.decode_png(self._decode_png_data, channels=3)\n",
    "\n",
    "    def read_image_dims(self, sess, image_data):\n",
    "        image = self.decode_png(sess, image_data)\n",
    "        return image.shape[0], image.shape[1]\n",
    "\n",
    "    def decode_png(self, sess, image_data):\n",
    "        image = sess.run(self._decode_png,\n",
    "                     feed_dict={self._decode_png_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "\n",
    "def _get_filenames_and_classes(dataset_dir):\n",
    "    directories = []\n",
    "    class_names = []\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        path = os.path.join(dataset_dir, filename)\n",
    "        if os.path.isdir(path):\n",
    "            directories.append(path)\n",
    "            class_names.append(filename)\n",
    "\n",
    "    photo_filenames = []\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            path = os.path.join(directory, filename)\n",
    "            photo_filenames.append(path)\n",
    "\n",
    "    return photo_filenames, sorted(class_names)\n",
    "\n",
    "def _dataset_exists(dataset_dir):\n",
    "    for split_name in ['train', 'validation']:\n",
    "        output_filename = _get_dataset_filename(dataset_dir, split_name)\n",
    "        if not tf.gfile.Exists(output_filename):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _get_dataset_filename(dataset_dir, split_name):\n",
    "    output_filename = '%s.tfrecord' % (split_name)\n",
    "    return os.path.join(dataset_dir, output_filename)\n",
    "\n",
    "\n",
    "def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n",
    "    assert split_name in ['train', 'validation']\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        image_reader = PngImageReader()\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            output_filename = _get_dataset_filename(dataset_dir, split_name)\n",
    "            with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "                files_written=0\n",
    "                for filename in tqdm(filenames):\n",
    "                    image_data = tf.gfile.FastGFile(filename, 'rb').read()\n",
    "                    height, width = image_reader.read_image_dims(sess, image_data)\n",
    "\n",
    "                    class_name = os.path.basename(os.path.dirname(filename))\n",
    "                    class_id = class_names_to_ids[class_name]\n",
    "                    \n",
    "                    example = dataset_utils.image_to_tfexample(image_data, b'png', height, width, class_id)\n",
    "                    tfrecord_writer.write(example.SerializeToString())\n",
    "                    files_written+=1\n",
    "            print('%d files written for %s' % (files_written, split_name))\n",
    "                    \n",
    "def generate_dataset(dataset_dir):\n",
    "    if not tf.gfile.Exists(dataset_dir):\n",
    "        tf.gfile.MakeDirs(dataset_dir)\n",
    "\n",
    "    if _dataset_exists(dataset_dir):\n",
    "        print('Dataset files already exist. Exiting without re-creating them.')\n",
    "        return\n",
    "\n",
    "    #dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n",
    "    photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n",
    "    class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n",
    "\n",
    "    # Divide into train and test:\n",
    "    random.seed(_RANDOM_SEED)\n",
    "    random.shuffle(photo_filenames)\n",
    "    training_filenames = photo_filenames[_NUM_VALIDATION:]\n",
    "    validation_filenames = photo_filenames[:_NUM_VALIDATION]\n",
    "\n",
    "    # First, convert the training and validation sets.\n",
    "    _convert_dataset('train', training_filenames, class_names_to_ids,\n",
    "                   dataset_dir)\n",
    "    _convert_dataset('validation', validation_filenames, class_names_to_ids,\n",
    "                   dataset_dir)\n",
    "\n",
    "    # Finally, write the labels file:\n",
    "    labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n",
    "    dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n",
    "    return training_filenames, validation_filenames, class_names_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"data/160x160_faces\"\n",
    "training_filenames, validation_filenames, class_names_to_ids = generate_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
